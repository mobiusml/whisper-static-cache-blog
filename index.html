<html>
<!DOCTYPE html>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
	<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css"> -->
	<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"> -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>	
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
	<script>hljs.highlightAll();</script>
	<script>
		function copyCode() {
		  const codeElement = document.querySelector('pre code');
		  const codeText = codeElement.innerText;
		  navigator.clipboard.writeText(codeText);
		  // Optionally provide user feedback, e.g., changing the button text
		}
	  </script>

	<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/latest.min.js"></script> -->

	<title>Speeding up Whisper</title>
	<link rel="stylesheet" type="text/css" href="styling.css">
	<link rel="icon" type="image/png" href="figs/aana_logo.png">	
	<link rel="stylesheet" href="https://use.typekit.net/pnf5khj.css">
	<!-- <link href='https://fonts.googleapis.com/css?family=Poppins' rel='stylesheet'> -->
	<link href="https://fonts.googleapis.com/css2?family=Merriweather:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">


	<!-- <link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Oxygen&family=Source+Serif+4:ital,opsz,wght@0,8..60,200..900;1,8..60,200..900&display=swap" rel="stylesheet"> -->


	<meta name="description" content="Fast and Small Whisper: The Power of Optimized Kernels and Quantization">

	<meta name="keywords"
		content="Whisper, ASR, Automatic Speech Recongition,  Machine Learning, Transformer Models, Neural Networks, AI Optimization, Faster Whisper, Quantization, HQQ">


	<!-- Specific tags for Open Graph / social media sharing -->
	<meta property="og:title" content="Fast and Small Whisper: The Power of Optimized Kernels and Quantization">
	<meta property="og:description"
		content="A support blog for speeding up whisper by batch processing.">
	<meta property="og:image" content="https://mobiusml.github.io/batched_whisper_blog/figs/aana_batch_whisper.png">
	<meta property="og:url" content="https://mobiusml.github.io/batched_whisper_blog/">
	<meta property="og:type" content="article">

	<!-- Twitter Card data -->
	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="Speeding up Whisper (ASR)"">
	<meta name="twitter:description"
		content="A support blog for the release of 1-bit Aana model.">
	<meta name="twitter:image" content=https://mobiusml.github.io/batched_whisper_blog/figs/aana_batch_whisper.png">
	<meta name="twitter:creator" content="@appughar">

	<!-- Meta tags for article publishing date and modification date -->
	<meta name="article:published_time" content="2024-03-27T08:00:00+00:00">
	<meta name="article:modified_time" content="2024-03-27T09:00:00+00:00">
</head>

<body>
	<article id="speeding up whisper" class="page sans">
		<header>
			<h1 class="page-title">Fast and Small Whisper: The Power of Optimized Kernels and Quantization</h1>

		</header>
		<div class="page-body">
			<p><a href="https://scholar.google.de/citations?user=KcoTPWoAAAAJ&hl=en"><mark
						class="highlight-gray">Jilt Sebastian</mark></a><mark class="highlight-gray">, </mark>,
						<a href="https://github.com/huseinzol05"><mark
							class="highlight-gray">Husein Zolkepli</mark></a><mark class="highlight-gray">, </mark>,</a>
						<a href="https://scholar.google.com/citations?user=LxweMX4AAAAJ&hl=en"><mark
						class="highlight-gray">Hicham Badri</mark></a><mark class="highlight-gray">, </mark>,
						<a
					href="https://scholar.google.com/citations?user=HxZDDzUAAAAJ&hl=en"><mark
						class="highlight-gray">Appu Shaji</mark></a><mark class="highlight-gray"></mark></p>
			<p><mark class="highlight-gray"><a href="https://www.mobiuslabs.com/"><mark class="highlight-gray">Mobius
							Labs GmbH</mark></a> and <a href="https://mesolitica.com/">Mesolitica</a></p>
			<hr />
			<h2 id="intro" class="">Introduction</h2>
			<p>Recent years have witnessed remarkable advancements in artificial intelligence, propelling rapid growth in automatic speech recognition (ASR) technologies. Soon after its release, OpenAI's Whisper model quickly gained prominence due to its open licensing, competitive performance against proprietary models, and strong generalization capabilities. Despite being in the field for over two years, Whisper models continue to be highly relevant and are the go-to workhorse for many large-scale ASR systems deployed worldwide.</p>

			<p>In this blog post, we explain the techniques we used to enhance the performance of the PyTorch based Whisper models. By leveraging transformers, implementing a static cache, and utilizing torch.compile, we significantly accelerated the model's inference speed. Additionally, we employed HQQ to quantize the Whisper models to 4 bits, maintaining a very low loss of quality in Word Error Rate (WER) benchmarks. Our optimizations resulted in a 4.5x speedup for non-quantized models and an impressive 6x speedup for quantized models.</p>

			<p>Furthermore, we provide benchmarks across various Automatic Speech Recognition (ASR) datasets to demonstrate the effectiveness of our optimizations. This post will delve into the methods and processes behind these improvements, providing insights into the power of optimized kernels and quantization.</p>

				
			<hr id="header_seperator" />
			<div class="column-list">
				<div style="width:32%" class="column">
					<!-- <p class="page-description"><img src="./baby_aana.png" /></p> -->
					<figure class="image" style="text-align:left"><a href="figs/aana_batch_whisper.png"><img style="width:240px"
								src="figs/aana_batch_whisper.png" /></a>
					</figure>
					<p>
						<strong>Table of Contents</strong>
					</p>
					<nav class="block-color-gray table_of_contents">
						<div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
								href="#intro">Introduction</a></div>
						<div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
								href="#speed-optimization">Speed Optimization</a>	
						</div>						
						<div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
							href="#quantization">Quantization</a>	
					</div>						
						<div class="table_of_contents-item table_of_contents-indent-0"><a
							class="table_of_contents-link" href="#benchmarks">Benchmarks</a>
						</div>						
						<div class="table_of_contents-item table_of_contents-indent-0"><a
								class="table_of_contents-link" href="#conclusion">Conclusion</a>
						</div>

						<hr />

						<p><strong> Test it out at </strong></p>
						<ul style="list-style-type:none;"> 
						 <p><a target="_blank" href="https://github.com/mobiusml/faster-whisper">
							<img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.svg" alt="Batched Faster Whisper" width="120"></a></p>
						<p><a target="_blank" href="https://colab.research.google.com/drive/1ywpZVj1NwZ2Tre0KOb_a2J_2S1hbzx0P?usp=sharing">
							<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
							</a></p>							
						

						<hr />
						

						<p><strong> Talk to us at </strong></p>
						<a href="https://discord.gg/zyZmwqQW"><img
							src="https://icons.iconarchive.com/icons/bootstrap/bootstrap/48/Bootstrap-discord-icon.png"
							width="24"></a>
						<a href="https://twitter.com/Mobius_Labs"><img
							src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/X_logo_2023.svg/450px-X_logo_2023.svg.png"
							width="24"></a>
					
						<hr />
							
						
					</nav>

				</div>
				<div style="width:75%" class="column">
			
					
					<h2 id="speed-optimization" class="">Speed Optimization </h2>
					<p><a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile</a> compiles PyTorch models into optimized kernels and can often result in significant speedups for various PyTorch-based models. One of the prerequisites and critical steps toward achieving this is managing a static kv-cache, as detailed in <a href="https://pytorch.org/blog/accelerating-generative-ai-2/">https://pytorch.org/blog/accelerating-generative-ai-2/</a>. During inference, the kv-cache stores activations. However, due to varying prompts and generation lengths, these caches can have dynamic lengths, which creates overhead. Instead, torch.compile methods require caches to be maintained statically by allocating the maximum size of the kv-cache and masking out unused values. The first step we took was to implement a static cache for the Whisper model to make it possible to use JIT compile backends. A gist of the implementation is available <a href="https://gist.github.com/huseinzol05/9aff34ec1427ee8c92240cb4f3cc0c88">here</a>. This is the first attempt at introducing a static cache for Encoder-Decoder models.</p>

					
					<h2 id="quantization" class="">Quantization</h2>
					<p>We further quantized models to 4 bits using <a href="https://github.com/mobiusml/hqq">Half-Quadratic Quantization (HQQ)</a>. HQQ is a fast and accurate model quantizer that does not need any calibration data. Additionally, it supports torch.compile backends. Though quantization techniques can reduce VRAM requirements, they often suffer from slower speed due to additional dequantization steps. However, recent developments have introduced new kernels that can perform quantized operations directly on the GPU by bit packing (<a href="https://pytorch.org/cppdocs/api/function_namespaceat_1adeda9630914278ac02d7fd758da19e3d.html">source</a>), and HQQ can leverage these kernels (<a href="https://github.com/mobiusml/hqq/blob/aad68687e042ed628b5a655969406d501a203949/hqq/backends/torchao.py#L295">source</a>). Note that these kernels are optimized for 4-bit GPUs with Ampere architecture (A100, A6000, RTX 3090, RTX 4090).</p>

					<p>Our previous experience with quantizing LLMs has shown that there is only a very marginal drop in quality (measured as word error rate) when quantizing to 4 bits, while achieving further speedup compared to vanilla PyTorch compiled models. Moreover, the GPU VRAM requirements can be reduced by a factor of 3 to 4.</p>
								
					<h2 id="benchmarks"">Benchmarks</h2>
					<p>We conduct two sets of benchmarking experiments to validate the speed-up using torch.compile and HQQ. The hypothesis is that since the major inference time is contributed by the decoding part, these modifications significantly reduces the overall processing time. The <a href="https://huggingface.co/datasets/open-asr-leaderboard/datasets"> Open ASR eval dataset </a> is used in the first experiment to measure the speed improvement and effect on the WER. The <a href="https://openvoice-tech.net/index.php/Real-time-factor">Real Time Factor (RTF)</a> metric, commonly used in open-source benchmarks like <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">Open ASR Leaderboard</a> evaluation, measures the speed of offline ASR systems by comparing total processing time to audio duration. We use 1/RTF in our benchmarking experiments as it is more interpretable once the speed becomes faster than the real-time speed, which is a common offline ASR requirement. 
					</p>
					<p>These datasets represent short-form audio as the average duration of each sample in the dataset is less than 10 seconds. Four Open ASR eval datasets are used with audios less than 30 sec for the experiments. We used the 'large-v2' model with batch_size of 1 and greedy decoding (no sampling) for the experiments. 
					</p>
						<table>
						<caption>Speed up and WER (short audio): The speed metric indicates how many times faster than real-time speed. The experiments are run on a RTX 4090 GPU.</caption>
						<tr>
							<th>Dataset</th>
							<th>Audio Length (Avg and STD)</th>						
							<th>Baseline </th>
							<th>Torch Compile</th>
							<th>Torch Compile + HQQ</th>
						</tr>
						<tr>
							<td>TEDLIUM</td>
							<td>8 secs (4)</td>							
							<td>11.6x (4.06%)</td>
							<td>35.3x (4.06%)</td>
							<td>37.3x (4.0%)</td>							
						</tr>
						<tr>
							<td>Voxpopuli</td>
							<td>10 secs (8)</td>							
							<td>14.2x (7.84%)</td>
							<td>42.6x (7.82%)</td>
							<td>47.9x (7.91%)</td>
						</tr>
						<tr>
							<td>Earnings22</td>
							<td>7 secs (5)</td>														
							<td>12.8x (12.13%)</td>
							<td>36.4x (12.15%)</td>
							<td>42.9x (12.83%)</td>
						</tr>
						<tr>
							<td>AMI</td>
							<td>2 secs (3)</td>														
							<td>9.8x (16.67%)</td>
							<td>20.8x (16.71%)</td>
							<td>20.4x (16.69%)</td>
						</tr>
					</table>
				<p> In order to check the effect of these decoding methods in real-world scenarios with long audios, we used an internal test dataset with 84 minutes duration and verified ground truth. The test set contains 9 audios ranging from 3 minutes to 13 minutes and various audio types.</p>
				<table>
					<caption>Speed up and WER (long form audio)</caption>
		
				<tr>
					<th>System</th>					
					<th>Baseline </th>
					<th>Torch Compile</th>
					<th>Torch Compile + HQQ</th>
				</tr>
				<tr>
					<td>Speed</td>
					<td>15.8x </td>
					<td>57.6x </td>
					<td>72.8x </td>							
				</tr>
				<tr>
					<td>WER (%)</td>
					<td>11.45</td>
					<td>11.47</td>
					<td>11.49</td>
				</tr>
				<tr>
					<td> Tokens/sec </td>
					<td>49.5</td>
					<td>243.4</td>
					<td>316.4</td>
				</tr>
				<tr>
					<td> VRAM usage </td>
					<td>8630 MiB</td>
					<td>8630 MiB</td>
					<td>3600 MiB</td>
				</tr>
			</table>

			<p> The speed up in the actual processing time depends on the audio length. For datasets with larger audio lengths, the speed up is higher as there are more tokens to decode at a higher pace. This is evident in AMI corpus, where the speed up is around </b>2x </b> vs  the long form audio where the speed up is <b>4.6x</b> for the HQQ based approach. Note that the actual speed up in the decoding speed is <b>4.9x </b> for the torch.compile and <b>6.4x</b> for the torch compiled version of HQQ. </p>
					
					<h2 id="citations">Citation</h2>
					<div>								
					<pre><code style="background-color: #fff; color: #777;" >
@misc{sebastian2024whisper1,
title = {Fast and Small Whisper: The Power of Optimized Kernels and Quantization},
url = {https://mobiusml.github.io/whisper-static-cache-blog/},
author = {Jilt Sebastian, Husein Zolkepli, Hicham Badri, and Appu Shaji},
month = {May},
year = {2024}
}
					</code></pre>
							</div>


							<div>
								<p style="text-align: center;">Please feel free to <a
										href="mailto:jilt@mobiuslabs.com">contact us.</a>.</p>
								<!--                             <p style="text-align: center; color:hotpink;">Check out our other blog post</p> -->

							</div>

				</div>




			</div>
			<p id="d9be7859-86c8-4e9e-8957-b0127ad9431d" class="">
			<div class="indented">
				<p id="7b0d7f13-0909-4e80-97fe-e0102053cc62" class="">
				</p>
			</div>
			</p>
		</div>
	</article>
</body>

</html>
