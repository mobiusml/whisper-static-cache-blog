<html>
<!DOCTYPE html>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
	<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css"> -->
	<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"> -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>	
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
	<script>hljs.highlightAll();</script>
	<script>
		function copyCode() {
		  const codeElement = document.querySelector('pre code');
		  const codeText = codeElement.innerText;
		  navigator.clipboard.writeText(codeText);
		  // Optionally provide user feedback, e.g., changing the button text
		}
	  </script>

	<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/latest.min.js"></script> -->

	<title>Faster and Smaller Whisper: A Deep Dive into Quantization and Torch Compilation</title>
	<link rel="stylesheet" type="text/css" href="styling.css">
	<link rel="icon" type="image/png" href="figs/aana_logo.png">	
	<link rel="stylesheet" href="https://use.typekit.net/pnf5khj.css">
	<!-- <link href='https://fonts.googleapis.com/css?family=Poppins' rel='stylesheet'> -->
	<link href="https://fonts.googleapis.com/css2?family=Merriweather:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">


	<!-- <link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Oxygen&family=Source+Serif+4:ital,opsz,wght@0,8..60,200..900;1,8..60,200..900&display=swap" rel="stylesheet"> -->


	<meta name="description" content="Fast and Small Whisper: The Power of Optimized Kernels and Quantization">

	<meta name="keywords"
		content="Whisper, ASR, Automatic Speech Recongition,  Machine Learning, Transformer Models, Neural Networks, AI Optimization, Faster Whisper, Quantization, HQQ">


	<!-- Specific tags for Open Graph / social media sharing -->
	<meta property="og:title" content="Fast and Small Whisper: The Power of Optimized Kernels and Quantization">
	<meta property="og:description"
		content="A support blog for speeding up whisper by batch processing.">
	<meta property="og:image" content="https://mobiusml.github.io/batched_whisper_blog/figs/aana_batch_whisper.png">
	<meta property="og:url" content="https://mobiusml.github.io/batched_whisper_blog/">
	<meta property="og:type" content="article">

	<!-- Twitter Card data -->
	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="Speeding up Whisper (ASR)">
	<meta name="twitter:description"
		content="A support blog for the release of fast and small whisper model.">
	<meta name="twitter:image" content=https://mobiusml.github.io/batched_whisper_blog/figs/aana_batch_whisper.png">
	<meta name="twitter:creator" content="@appughar">

	<!-- Meta tags for article publishing date and modification date -->
	<meta name="article:published_time" content="2024-03-27T08:00:00+00:00">
	<meta name="article:modified_time" content="2024-03-27T09:00:00+00:00">
</head>

<body>
	<article id="speeding up whisper" class="page sans">
		<header>
			<h1 class="page-title">Faster and Smaller Whisper: A Deep Dive into Quantization and Torch Compilation</h1>

		</header>
		<div class="page-body">
			<p><a href="https://scholar.google.de/citations?user=KcoTPWoAAAAJ&hl=en"><mark
						class="highlight-gray">Jilt Sebastian</mark></a><mark class="highlight-gray">, </mark>,
						<a href="https://github.com/huseinzol05"><mark
							class="highlight-gray">Husein Zolkepli</mark></a><mark class="highlight-gray">, </mark>,</a>
						<a href="https://scholar.google.com/citations?user=LxweMX4AAAAJ&hl=en"><mark
						class="highlight-gray">Hicham Badri</mark></a><mark class="highlight-gray">, </mark>,
						<a
					href="https://scholar.google.com/citations?user=HxZDDzUAAAAJ&hl=en"><mark
						class="highlight-gray">Appu Shaji</mark></a><mark class="highlight-gray"></mark></p>
			<p><mark class="highlight-gray"><a href="https://www.mobiuslabs.com/"><mark class="highlight-gray">Mobius
							Labs GmbH</mark></a> and <a href="https://mesolitica.com/">Mesolitica</a></p>
			<hr />
			<h2 id="intro" class="">Introduction</h2>
			<p>Recent years have witnessed remarkable advancements in artificial intelligence, propelling rapid growth in automatic speech recognition (ASR) technologies. Soon after its release, OpenAI's <a href="https://github.com/openai/whisper">Whisper</a> model quickly gained prominence due to its open licensing, competitive performance against proprietary models, and strong generalization capabilities. Despite being in the field for over two years, Whisper models continue to be highly relevant and are the go-to workhorse for many large-scale ASR systems deployed worldwide.</p>

			<p>In this blog post, we explain the techniques we used to enhance the performance of the PyTorch based Whisper models. By leveraging transformers, implementing a <a href"https://github.com/huggingface/transformers/pull/30760"> static cache</a>, and utilizing <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile</a>, we significantly accelerated the model's inference speed. Additionally, we employed <a href="https://github.com/mobiusml/hqq">HQQ</a> to quantize the Whisper models to 4 bits, maintaining a very low loss of quality in Word Error Rate (WER) benchmarks. Our optimizations resulted in a <b>4.5x</b> speedup for non-quantized models and an impressive <b>6x</b> speedup for quantized models.</p>

			<p>Furthermore, we provide benchmarks across various Automatic Speech Recognition (ASR) datasets to demonstrate the effectiveness of our optimizations. This post will delve into the methods and processes behind these improvements, providing insights into the power of optimized kernels and quantization.</p>



				
			<hr id="header_seperator" />
			<div class="column-list">
				<div style="width:32%" class="column">
					<!-- <p class="page-description"><img src="./baby_aana.png" /></p> -->
					<figure class="image" style="text-align:left"><a href="figs/aana_batch_whisper.png"><img style="width:240px"
								src="figs/aana_batch_whisper.png" /></a>
					</figure>
					<p>
						<strong>Table of Contents</strong>
					</p>
					<nav class="block-color-gray table_of_contents">
						<div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
								href="#intro">Introduction</a></div>
						<div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
								href="#speed-optimization">Speed Optimization</a>	
						</div>						
						<div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
							href="#quantization">Quantization</a>	
					</div>						
						<div class="table_of_contents-item table_of_contents-indent-0"><a
							class="table_of_contents-link" href="#benchmarks">Benchmarks</a>
						</div>						
						<div class="table_of_contents-item table_of_contents-indent-0"><a
								class="table_of_contents-link" href="#conclusion">Conclusion</a>
						</div>

						<hr />

						<!-- <p><strong> Test it out at </strong></p>
						<ul style="list-style-type:none;"> 
						 <p><a target="_blank" href="https://github.com/mobiusml/faster-whisper">
							<img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.svg" alt="Batched Faster Whisper" width="120"></a></p>
						<p><a target="_blank" href="https://colab.research.google.com/drive/1ywpZVj1NwZ2Tre0KOb_a2J_2S1hbzx0P?usp=sharing">
							<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
							</a></p>							
						

						<hr />
						 -->

						<p><strong> Talk to us at </strong></p>
						<a href="https://discord.gg/zyZmwqQW"><img
							src="https://icons.iconarchive.com/icons/bootstrap/bootstrap/48/Bootstrap-discord-icon.png"
							width="24"></a>
						<a href="https://twitter.com/Mobius_Labs"><img
							src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/X_logo_2023.svg/450px-X_logo_2023.svg.png"
							width="24"></a>
					
						<hr />
							
						
					</nav>

				</div>
				<div style="width:75%" class="column">
			
					
					<h2 id="speed-optimization" class="">Speed Optimization </h2>
					<p><a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile</a> compiles PyTorch models into optimized Triton kernels and can often result in significant speedups for various PyTorch-based models. One of the prerequisites and critical steps toward achieving this is managing a static KV-cache, as detailed in <a href="https://pytorch.org/blog/accelerating-generative-ai-2/">https://pytorch.org/blog/accelerating-generative-ai-2/</a>. During inference, the KV-cache stores intermediate outputs of the model in the attention modules. However, due to varying prompts and generation lengths, these caches are implemented via <a href="https://github.com/openai/whisper/blob/ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab/whisper/model.py#L301">dynamic allocation</a>, which creates overhead. Instead, torch.compile requires the KV-cache to be maintained statically by pre-allocating the maximum size. </p>
					
					<p>The first step we took was to implement a static cache for the Whisper model to make it fully compatible with torch.compile. A gist of the implementation is available <a href="https://gist.github.com/huseinzol05/9aff34ec1427ee8c92240cb4f3cc0c88">here</a>. As far as we know, this is the first implementation of a <a href="https://github.com/huggingface/transformers/pull/30760#discussion_r1606974803">static cache for encoder-decoder models in the Transformers library</a>, and hopefully, it will be useful for other encoder-decoder architectures. </p>

					
					<h2 id="quantization" class="">Quantization</h2>
					<p>We further quantized models to 4 bits using <a href="https://github.com/mobiusml/hqq">Half-Quadratic Quantization (HQQ)</a>. HQQ is a fast and accurate model quantizer that does not need any calibration data. Additionally, it fully supports torch.compile. Though quantization techniques can reduce VRAM requirements, they require optimized fused kernels to run faster. Recent developments have introduced low-bit matmul kernels that offer significant speed-ups such as TorchAO's <a href="https://pytorch.org/cppdocs/api/function_namespaceat_1adeda9630914278ac02d7fd758da19e3d.html">tiny_gemm</a>, and HQQ <a href="https://github.com/mobiusml/hqq/blob/aad68687e042ed628b5a655969406d501a203949/examples/backends/torchao_int4_demo.py">can leverage these kernels</a>). </p>

					<p>Our previous experience with quantizing LLMs has shown that there is only a very marginal drop in quality (measured as word error rate) when quantizing to 4 bits, while achieving further speedup compared to vanilla PyTorch compiled models. Moreover, the GPU VRAM requirements for the decoder can be reduced by a factor of 3 to 4.</p>

					<p>We only quantize the decoder layer of Whisper for the following reason: The encoder layer involves a lot of matrix-matrix computations (GEMM), whereas the decoder layers primarily involve matrix-vector multiplications (GEMV). The TorchAO kernels are optimized to speed up GEMV operations. Therefore, quantizing the encoder layer would not leverage a speed-up.</p>  <!-- and would result in inferior performance.-->

					<p>Even if the encoder layerâ€™s performance is optimized, its impact on the overall run time is marginal compared to the decoder layer. The encoder layer includes only one forward pass, whereas the decoder layers must be run many times, once per token generated.</p>
								
					<h2 id="benchmarks">Benchmarks</h2>
					<p>We conducted two sets of benchmarking experiments to validate the speed-up using torch.compile and HQQ. The <a href="https://huggingface.co/datasets/open-asr-leaderboard/datasets">Open ASR eval dataset</a> is used in the first experiment to measure the speed improvement and effect on the WER, and a test set of audio extracts from YouTube videos is used in the second benchmarking. We find that in real-life use cases, transcripts are generally long, hence the second dataset is included. We used the 'large-v2' model with a batch size of 1 and greedy decoding (no sampling) for the experiments.</p>
						
					<p>The <a href="https://openvoice-tech.net/index.php/Real-time-factor">Real Time Factor (RTF)</a> metric, commonly used in open-source benchmarks like <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">Open ASR Leaderboard</a> evaluation, measures the speed of offline ASR systems by comparing total processing time to audio duration. We use 1/RTF in our benchmarking experiments as it is more interpretable once the speed becomes faster than the real-time speed, which is a common offline ASR requirement. We used 'EnglishTextNormalizer()' from <a href="https://github.com/huggingface/open_asr_leaderboard">Open ASR Eval</a> as the text normalizer to compute WER. Experiments use torch nightly build as of 28 May 2024. The experiments are run on a RTX 4090 GPU.
					</p>


					<h3>Short-form Audio</h3>
					<p>The first dataset represents short-form audio as the average duration of each sample in the dataset is less than 10 seconds. Four Open ASR eval datasets with audios less than 30 seconds are used for the experiments. Baseline system is the current implementation in <a href="https://github.com/huggingface/transformers" ></a>transformers, that does not use torch.compile operation. </p>
						<table>
						<caption> Speed up and WER (short form audio). The speed metric indicates how many times faster than the real-time speed. </caption>
						<tr>
							<th>Dataset</th>
							<th>Avg. Duration</th>						
							<th colspan="2">Baseline</th>
							<th colspan="2">Torch Compile</th>
							<th colspan="2">Torch Compile + HQQ</th>
						</tr>
						<tr>
							<th></th>
							<th></th>						
							<th>Speed</th>
							<th>WER</th>
							<th>Speed</th>
							<th>WER</th>
							<th>Speed</th>
							<th>WER</th>
							
						</tr>				
						<tr>
							<td><a href="https://huggingface.co/datasets/LIUM/tedlium">TEDLIUM</a></td>
							<td>8 secs</td>							
							<td>11.6x</td>
							<td>4.06%</td>
							<td>35.3x </td>
							<td>4.06%</td>
							<td>37.3x</td>							
							<td>4.0%</td>
						</tr>
						<tr>
							<td><a href="https://huggingface.co/datasets/facebook/voxpopuli">Voxpopuli</a></td>
							<td>10 secs</td>							
							<td>14.2x</td>
							<td>7.84%</td>
							<td>42.6x</td>
							<td>7.84%</td>
							<td>47.9x</td>
							<td>7.91%</td>
						</tr>
						<tr>
							<td><a href="https://huggingface.co/datasets/distil-whisper/earnings22">Earnings22</a></td>
							<td>7 secs</td>														
							<td>12.8x</td>
							<td>12.13%</td>
							<td>36.4x</td>
							<td>12.15%</td>
							<td>42.9x</td>
							<td>12.83%</td>
						</tr>
						<tr>
							<td><a href="https://huggingface.co/datasets/edinburghcstr/ami">AMI</a></td>
							<td>2 secs</td>														
							<td>9.8x</td>
							<td>16.67%</td>
							<td>20.8x </td>
							<td>16.71%</td>
							<td>20.4x </td>
							<td>16.69%</td>
						</tr>
					</table>

				<h3>Real-word Scenario: Long Audios</h3>
				<p> In order to check the effect of these decoding methods in real-world scenarios with long audios, we used an internal test dataset with 84 minutes duration and verified ground truth. The test set contains 9 audios ranging from 3 minutes to 13 minutes and covers various audio types.</p>
				<table>
					<caption>Speed up and WER (long form audio). We also compare the VRAM usage of the models.</caption>
		
				<tr>
					<th>System</th>					
					<th>Baseline </th>
					<th>Torch Compile</th>
					<th>Torch Compile + HQQ</th>
				</tr>
				<tr>
					<td>Speed</td>
					<td>15.8x </td>
					<td>57.6x </td>
					<td>72.8x </td>			
				</tr>
				<tr>
					<td>WER (%)</td>
					<td>11.45</td>
					<td>11.47</td>
					<td>11.49</td>
				</tr>
				<!-- <tr>
					<td> Tokens/sec </td>
					<td>49.5</td>
					<td>243.4</td>
					<td>316.4</td>
				</tr> -->
				<tr>
					<td> VRAM usage </td>
					<td>3576 MiB</td>
					<td>3576 MiB</td>
					<td>2454 MiB</td>
				</tr>
			</table>

			<p>The speed-up in the actual processing time depends on the audio length. For datasets with larger audio lengths, the speed-up is higher as there are more tokens to decode at a higher pace, and the impact of optimization and quantization is more profound.</p> 
			
			<p>For example in AMI corpus, which is a corpus with very short audio clips with average length of 2 seconds, the speed up is around </b>2x</b> compared to baseline versus the long form dataset where the speed up is <b>4.6x</b> for the HQQ based approach and <b>3.6x</b> for pytorch compiled one. This speed up can be achieved on top of the fastest sequential /batched execution pytorch based models, resulting in ultra-fast ASR inference. </p>

			<h4>Tokens per Second</h4>

			<div class="column-list">
				<div style="width:49%" class="column">
					<div class="caption">Baseline</div>
					<img src="figs/transcription_speed_default_opt.gif" />
					
				</div>
				<div style="width:49%" class="column">
					<div class="caption">Torch Compile + HQQ</div> 
					<img src="figs/transcription_compile_hqq_opt.gif" />					
				</div>

			</div>

			<p>The significant speed-up and impact of optimization and quantization happens in the decoder layers. The following table shows the tokens per second processed by the decoder.</p>

			<table>
				<tr>
					<td>Baseline</td>
					<td>49.5 it/sec</td>
				</tr>
				<tr>
					<td>Torch Compile</td>
					<td>243.4 it/sec</td>
				</tr>
				<tr>
					<td>Torch Compile + HQQ</td>
					<td>316.4 it/sec</td>
				</tr>
			</table>


			<h3>Whisper in Extreme Low-Bit Configurations</h3>

			<p>Additionally, we tested Whisper in extreme low-bit configurations (i.e., 3, 2 and 1.58 (ternary) bits) with different quantisation group sizes. Note that the PyTorch acceleration kernels we use are optimized for 4 bits, so the speed decreases when the bit size is less than 4 bits. Below, we report the Word Error Rate (WER) for these configurations:</p>

			
			<table>
				<caption> WER for low-bit quantized Whisper Models. </caption>
				<tr>
					<th>Dataset</th>
					<th>#bits</th>
					<th>Group Size</th>
					<th>WER</th>						
					<th>Speed</th>					
				</tr>
				<tr>
					<td rowspan="9">TEDLIUM</td>
					<td class="group2">4 bit</td>
					<td class="group2">64</td>
					<td class="group2">4.06</td>
					<td class="group2">37.3x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>16</td>
					<td>4.94</td>
					<td>17.9x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>32</td>
					<td>4.14</td>
					<td>18.0x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>64</td>
					<td>4.72</td>
					<td>15.9x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>16</td>
					<td>4.61</td>
					<td>26.2x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>32</td>
					<td>4.77</td>
					<td>23.2x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>64</td>
					<td>7.26</td>
					<td>24.4x</td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>16</td>
					<td>5.30</td>
					<td>26.3x</td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>32</td>
					<td>23.94</td>
					<td>25.7x</td>
				</tr>
				<tr>
					<td rowspan="9">Voxpopuli</td>
					<td class="group2">4 bit</td>
					<td class="group2">64</td>
					<td class="group2">7.84</td>
					<td class="group2">47.9x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>16</td>
					<td>9.03</td>
					<td>20.7x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>32</td>
					<td>8.40</td>
					<td>21.3x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>64</td>
					<td>7.82</td>
					<td>21.8x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>16</td>
					<td>8.05</td>
					<td>29.6x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>32</td>
					<td>8.74</td>
					<td>30.0x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>64</td>
					<td>8.97</td>
					<td>29.4x</td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>16</td>
					<td>9.64</td>
					<td>27.8x</td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>32</td>
					<td>30.42</td>
					<td>25.3x</td>
				</tr>
				<tr>
					<td rowspan="9">Earnings22</td>
					<td class="group2">4 bit</td>
					<td class="group2">64</td>
					<td class="group2">12.15</td>
					<td class="group2">42.9x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>16</td>
					<td>12.3</td>
					<td>17.7x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>32</td>
					<td>12.41</td>
					<td>16.3x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>64</td>
					<td>12.26</td>
					<td>17.5x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>16</td>
					<td>13.60</td>
					<td>25.9x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>32</td>
					<td>13.54</td>
					<td>25.9x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>64</td>
					<td>16.03</td>
					<td>23.0x</td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>16</td>
					<td>12.40</td>
					<td>20.5x</td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>32</td>
					<td>38.50</td>
					<td>23x</td>
				</tr>
				<tr>
					<td rowspan="9">AMI</td>
					<td class="group2">4 bit</td>
					<td class="group2">64</td>
					<td class="group2">16.71</td>
					<td class="group2">20.4x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>16</td>
					<td>16.63</td>
					<td>9.0x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>32</td>
					<td>16.64</td>
					<td>8.6x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>64</td>
					<td>16.72</td>
					<td>8.7x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>16</td>
					<td>17.30</td>
					<td>13.4x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>32</td>
					<td>17.80</td>
					<td>13.7x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>64</td>
					<td>19.52</td>
					<td>12.9x</td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>16</td>
					<td>19.15</td>
					<td>13.3x</td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>32</td>
					<td>31.22</td>
					<td>12.7x</td>
				</tr>
				<tr">
					<td rowspan="12">Long-form Audio</td>
					<td class="group2" >4 bit</td>
					<td class="group2">64</td>
					<td class="group2">11.49</td>
					<td class="group2">72.8x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>					
					<td>64</td>
					<td>11.56</td>
					<td>34.6x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>32</td>
					<td>11.40</td>
					<td>33.9x</td>
				</tr>
				<tr class="group1">					
					<td>3 bit</td>
					<td>16</td>
					<td>11.56</td>
					<td>34.6x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>64</td>
					<td>12.74</td>
					<td>46.0x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>32</td>
					<td>11.75</td>
					<td>44.7x</td>
				</tr>
				<tr class="group2">
					<td>2 bit</td>
					<td>16</td>
					<td>11.58</td>
					<td>45.3x</td>
				</tr>
				
				<tr class="group1">
					<td>1.58 bit</td>
					<td>64</td>
					<td>96.66</td>
					<td>- <small>(higly erroneous)</small></td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>32</td>
					<td>41.8</td>
					<td>38.7x <small>(high hallucinations)</small></td>
				</tr>
				<tr class="group1">
					<td>1.58 bit</td>
					<td>16</td>
					<td>12.48</td>
					<td>42.9x</td>
				</tr>
			</table>

			<p>To our surprise, Whisper's performance only degrades gracefully and remains quite useful even in extreme low-bit configurations. For 2 bit and 1.58, best performance is achieved for a group size of 16. For larger bits, group size=64 performs the best.  Lower bits are quite promising since they are conducive to hardware implementation, which can result in energy efficiency upto 70x.</p>
			<p>This warrants two future directions we will be pursuing: the development of more optimized kernels for lower bits and the use of post-quantization training, proposed as one of the methods in our 1-bit blog, to gracefully recover from low-bit performance degradation.</p>

			<h3>Caveats</h3>

			<p>A few caveats to note</p>
			<ol>
				<li>The TorchAO kernels required modern GPUs: Amphere, RTX series and above.</li>
			</ol>



					
					<h2 id="citations">Citation</h2>
					<div>								
					<pre><code style="background-color: #fff; color: #777;" >
@misc{sebastian2024whisper1,
title = {Faster and Smaller Whisper: A Deep Dive into Quantization and Torch Compilation},
url = {https://mobiusml.github.io/whisper-static-cache-blog/},
author = {Jilt Sebastian, Husein Zolkepli, Hicham Badri, and Appu Shaji},
month = {May},
year = {2024}
}
					</code></pre>
							</div>


							<div>
								<p style="text-align: center;">Please feel free to <a
										href="mailto:jilt@mobiuslabs.com">contact us.</a>.</p>
								<!--                             <p style="text-align: center; color:hotpink;">Check out our other blog post</p> -->

							</div>

				</div>




			</div>
			<p id="d9be7859-86c8-4e9e-8957-b0127ad9431d" class="">
			<div class="indented">
				<p id="7b0d7f13-0909-4e80-97fe-e0102053cc62" class="">
				</p>
			</div>
			</p>
		</div>
	</article>
</body>

</html>
